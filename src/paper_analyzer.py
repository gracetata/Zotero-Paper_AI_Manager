"""
paper_analyzer.py â€” ä¸»å…¥å£è„šæœ¬
è®ºæ–‡åˆ†ææµç¨‹ç¼–æ’ï¼šè¯»å– Zotero æ¡ç›® â†’ æå– PDF â†’ LLM åˆ†æ â†’ ä¿å­˜è¾“å‡º

ç”¨æ³•:
  python paper_analyzer.py --all              # å¤„ç†æ‰€æœ‰æœªåˆ†æçš„è®ºæ–‡ï¼ˆå…¨åº“æ‰¹é‡ï¼‰
  python paper_analyzer.py --key ITEM_KEY     # å¤„ç†æŒ‡å®š Zotero æ¡ç›®
  python paper_analyzer.py --recent 5         # å¤„ç†æœ€è¿‘5ç¯‡è®ºæ–‡
  python paper_analyzer.py --dry-run          # ä»…é¢„è§ˆï¼Œä¸å†™å…¥
"""

import os
import sys
import json
import yaml
import argparse
import re
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ  src åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(__file__))

from zotero_client import ZoteroClient
from pdf_extractor import extract_all_pages, get_page_count
from github_models_client import GitHubModelsClient


# ---- é…ç½®åŠ è½½ ----

def load_config():
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config.yaml')
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


# ---- æ–‡ä»¶åæ¸…ç† ----

def safe_filename(title, max_len=80):
    """å°†è®ºæ–‡æ ‡é¢˜è½¬æ¢ä¸ºåˆæ³•æ–‡ä»¶å"""
    name = re.sub(r'[^\w\s\-]', '', title)
    name = re.sub(r'\s+', '_', name.strip())
    return name[:max_len]


# ---- Markdown è¾“å‡º ----

def save_analysis_markdown(analysis_text, metadata, output_dir):
    """
    ä¿å­˜åˆ†æç»“æœä¸º Markdown æ–‡ä»¶ã€‚
    è¿”å›ä¿å­˜è·¯å¾„ï¼ˆç›¸å¯¹äº notes ç›®å½•ï¼‰ã€‚
    """
    year = metadata.get('year') or datetime.now().strftime('%Y')
    year_dir = os.path.join(output_dir, str(year))
    os.makedirs(year_dir, exist_ok=True)

    fname = safe_filename(metadata.get('title', metadata['key'])) + '.md'
    filepath = os.path.join(year_dir, fname)

    header = f"""---
zotero_key: {metadata['key']}
title: "{metadata.get('title', '')}"
authors: "{metadata.get('authors', '')}"
year: "{metadata.get('year', '')}"
venue: "{metadata.get('venue', '')}"
doi: "{metadata.get('doi', '')}"
analyzed_at: "{datetime.now().isoformat()}"
---

"""
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(header + analysis_text)

    print(f"  âœ… Markdown å·²ä¿å­˜: {filepath}")
    return filepath


# ---- INDEX.md æ›´æ–° ----

def update_index(index_file, metadata, tags, analysis_path, notes_dir):
    """åœ¨ INDEX.md ä¸­æ·»åŠ æˆ–æ›´æ–°è®ºæ–‡è®°å½•"""
    # è®¡ç®—ç›¸å¯¹è·¯å¾„
    rel_path = os.path.relpath(analysis_path, notes_dir)
    tag_str = ' '.join([f'`{t}`' for t in tags]) if tags else ''
    year = metadata.get('year', '?')
    title = metadata.get('title', metadata['key'])
    authors = metadata.get('authors', '')
    # æˆªæ–­ä½œè€…ï¼ˆåªæ˜¾ç¤ºç¬¬ä¸€ä½œè€… et al.ï¼‰
    if '; ' in authors:
        first_author = authors.split('; ')[0]
        authors_short = f"{first_author} et al."
    else:
        authors_short = authors

    new_row = f"| [{title}]({rel_path}) | {authors_short} | {year} | {tag_str} |\n"

    if not os.path.exists(index_file):
        # åˆ›å»ºæ–°çš„ INDEX.md
        with open(index_file, 'w', encoding='utf-8') as f:
            f.write("# ğŸ“š è®ºæ–‡é˜…è¯»ç´¢å¼•\n\n")
            f.write(f"> è‡ªåŠ¨ç”Ÿæˆ Â· æœ€åæ›´æ–°: {datetime.now().strftime('%Y-%m-%d %H:%M')}\n\n")
            f.write("| æ ‡é¢˜ | ä½œè€… | å¹´ä»½ | æ ‡ç­¾ |\n")
            f.write("|------|------|------|------|\n")
            f.write(new_row)
        print(f"  âœ… INDEX.md å·²åˆ›å»º: {index_file}")
        return

    # è¯»å–ç°æœ‰å†…å®¹
    with open(index_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æ›´æ–°æ—¶é—´æˆ³
    content = re.sub(
        r'> è‡ªåŠ¨ç”Ÿæˆ Â· æœ€åæ›´æ–°: .+\n',
        f'> è‡ªåŠ¨ç”Ÿæˆ Â· æœ€åæ›´æ–°: {datetime.now().strftime("%Y-%m-%d %H:%M")}\n',
        content
    )

    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨æ­¤æ¡ç›®ï¼ˆé€šè¿‡ zotero_key æŸ¥æ‰¾ï¼‰
    key = metadata['key']
    if key in content:
        print(f"  â„¹ï¸  INDEX.md ä¸­å·²æœ‰æ­¤æ¡ç›® ({key})ï¼Œè·³è¿‡")
        return

    # åœ¨è¡¨æ ¼æœ«å°¾æ·»åŠ æ–°è¡Œ
    with open(index_file, 'w', encoding='utf-8') as f:
        f.write(content.rstrip() + '\n' + new_row)

    print(f"  âœ… INDEX.md å·²æ›´æ–°")


# ---- é˜…è¯»çŠ¶æ€å£°æ˜ï¼ˆåµŒå…¥åˆ†ææŠ¥å‘Šé¡¶éƒ¨ï¼‰----

def _build_read_status_note(read_ratio, actual_chars, original_chars, total_pages, model_name):
    """æ„å»ºé˜…è¯»çŠ¶æ€å£°æ˜ï¼ŒåµŒå…¥åœ¨åˆ†ææŠ¥å‘Šé¡¶éƒ¨"""
    if original_chars == 0:
        return f"> ğŸ¤– **åˆ†ææ¨¡å‹**: {model_name}  \n> âš ï¸ **è¯»å–çŠ¶æ€**: æœªæ‰¾åˆ° PDFï¼Œä»…åŸºäºå…ƒæ•°æ®å’Œæ‘˜è¦åˆ†æ"
    if read_ratio >= 0.99:
        return (
            f"> ğŸ¤– **åˆ†ææ¨¡å‹**: {model_name}  \n"
            f"> âœ… **è¯»å–çŠ¶æ€**: å·²è¯»å–å…¨æ–‡ï¼ˆ{actual_chars:,} å­—ç¬¦ / {total_pages} é¡µï¼‰"
        )
    pct = int(read_ratio * 100)
    return (
        f"> ğŸ¤– **åˆ†ææ¨¡å‹**: {model_name}  \n"
        f"> âš ï¸ **è¯»å–çŠ¶æ€**: ä»…è¯»å–äº†è®ºæ–‡å‰ **{pct}%** å†…å®¹"
        f"ï¼ˆ{actual_chars:,} / {original_chars:,} å­—ç¬¦ Â· {total_pages} é¡µï¼‰  \n"
        f"> ğŸ’¡ **æç¤º**: å¦‚éœ€å…¨æ–‡åˆ†æï¼Œå¯åœ¨ config.yaml ä¸­é…ç½® `anthropic.api_key` å¹¶ä½¿ç”¨ Claude æ¨¡å‹ï¼ˆ`--model claude-sonnet-4-6`ï¼‰"
    )


# ---- æ ¸å¿ƒæµç¨‹ ----

def process_item(item_key, zotero_client, llm_client, config, dry_run=False):
    """å¤„ç†å•ç¯‡è®ºæ–‡çš„å®Œæ•´åˆ†ææµç¨‹"""
    print(f"\n{'='*60}")
    print(f"ğŸ” æ­£åœ¨å¤„ç†: {item_key}")

    # 1. è·å– Zotero å…ƒæ•°æ®
    try:
        item = zotero_client.get_item(item_key)
        metadata = zotero_client.get_item_metadata(item)
    except Exception as e:
        print(f"  âŒ è·å– Zotero å…ƒæ•°æ®å¤±è´¥: {e}")
        return False

    print(f"  ğŸ“„ æ ‡é¢˜: {metadata['title']}")
    print(f"  ğŸ‘¤ ä½œè€…: {metadata['authors']}")
    print(f"  ğŸ“… å¹´ä»½: {metadata['year']}")

    # 2. æŸ¥æ‰¾å¹¶æå– PDF
    pdf_path = zotero_client.find_local_pdf(item_key)
    if not pdf_path:
        pdf_path = zotero_client.find_pdf_via_attachments(item_key)

    pdf_text = None
    original_pdf_chars = 0
    total_pages = 0
    if pdf_path:
        pdf_cfg = config.get('pdf', {})
        pdf_text, total_pages, was_truncated = extract_all_pages(
            pdf_path,
            max_chars=pdf_cfg.get('max_chars', 150000)
        )
        if pdf_text:
            original_pdf_chars = len(pdf_text)
            trunc_note = " âš ï¸ (æ–‡ä»¶è¶…å¤§ï¼Œå·²å®‰å…¨æˆªæ–­)" if was_truncated else "ï¼ˆå…¨æ–‡ï¼‰"
            print(f"  ğŸ“– PDF: {total_pages} é¡µï¼Œ{original_pdf_chars:,} å­—ç¬¦ {trunc_note}")
        else:
            print(f"  âš ï¸  PDF æå–å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨å…ƒæ•°æ®")
    else:
        print(f"  âš ï¸  æœªæ‰¾åˆ°æœ¬åœ° PDFï¼Œå°†ä»…ä½¿ç”¨å…ƒæ•°æ®")

    if dry_run:
        print(f"  [dry-run] è·³è¿‡ LLM è°ƒç”¨å’Œå†™å…¥æ“ä½œ")
        return True

    # 3. è°ƒç”¨ LLM åˆ†æï¼ˆè¿”å› analysis + å®é™…é˜…è¯»æ¯”ä¾‹ï¼‰
    print(f"  ğŸ¤– è°ƒç”¨ {llm_client.model} åˆ†æä¸­...")
    try:
        analysis, read_ratio, actual_chars = llm_client.analyze_paper(
            metadata, pdf_text, original_pdf_chars=original_pdf_chars
        )
    except RuntimeError as e:
        print(f"  âŒ LLM åˆ†æå¤±è´¥: {e}")
        return False

    # å‘ŠçŸ¥ç”¨æˆ·å®é™…é˜…è¯»äº†å¤šå°‘
    if pdf_text:
        if read_ratio >= 0.99:
            print(f"  âœ… å…¨æ–‡å·²è¯»å–ï¼ˆ{actual_chars:,} å­—ç¬¦ï¼Œ100%ï¼‰")
        elif read_ratio > 0:
            pct = int(read_ratio * 100)
            print(f"  âš ï¸  ä»…è¯»å–äº†è®ºæ–‡å‰ {pct}% å†…å®¹ï¼ˆ{actual_chars:,}/{original_pdf_chars:,} å­—ç¬¦ï¼‰"
                  f" â€” ååŠéƒ¨åˆ†æœªçº³å…¥åˆ†æï¼Œå»ºè®®åˆ‡æ¢æ›´å¤§ä¸Šä¸‹æ–‡æ¨¡å‹")
        else:
            print(f"  âš ï¸  æœªèƒ½è¯»å– PDFï¼Œåˆ†æä»…åŸºäºå…ƒæ•°æ®å’Œæ‘˜è¦")

    # åœ¨åˆ†ææ–‡æœ¬å¼€å¤´æ’å…¥é˜…è¯»çŠ¶æ€å£°æ˜
    read_status_note = _build_read_status_note(read_ratio, actual_chars, original_pdf_chars, total_pages, llm_client.model)
    analysis_with_note = read_status_note + '\n\n' + analysis

    # 4. æå–æ ‡ç­¾
    all_valid_tags = (
        config['tags'].get('domain', []) +
        config['tags'].get('method', []) +
        config['tags'].get('status', [])
    )
    tags = llm_client.extract_tags_from_analysis(analysis, valid_tags=all_valid_tags)
    if not any(t in config['tags'].get('status', []) for t in tags):
        tags.append('å·²è¯»')
    print(f"  ğŸ·ï¸  æ¨èæ ‡ç­¾: {tags}")

    # 5. ä¿å­˜ Markdownï¼ˆå«é˜…è¯»çŠ¶æ€å£°æ˜ï¼‰
    notes_dir = config['output']['notes_dir']
    analysis_path = save_analysis_markdown(analysis_with_note, metadata, notes_dir)

    # 6. æ›´æ–° INDEX.md
    update_index(config['output']['index_file'], metadata, tags, analysis_path, notes_dir)

    # 7. å°† Markdown ä»¥ã€Œé“¾æ¥æ–‡ä»¶ã€æ–¹å¼æŒ‚åˆ° Zotero æ¡ç›®
    try:
        att_key = zotero_client.add_linked_markdown(item_key, analysis_path)
        print(f"  âœ… Markdown å·²å…³è”åˆ° Zotero é™„ä»¶ (key: {att_key})")
    except FileNotFoundError as e:
        print(f"  âš ï¸  é™„ä»¶å…³è”å¤±è´¥: {e}")
    except RuntimeError as e:
        print(f"  âš ï¸  {e}")
    except Exception as e:
        print(f"  âš ï¸  Zotero é™„ä»¶å…³è”å¤±è´¥ï¼ˆä¸å½±å“å…¶ä»–å†™å…¥ï¼‰: {e}")

    # 8. å†™å…¥ Zotero ç¬”è®°
    try:
        zotero_client.add_note(item_key, analysis_with_note)
        print(f"  âœ… Zotero ç¬”è®°å·²å†™å…¥")
    except RuntimeError as e:
        print(f"  âš ï¸  {e}")
    except Exception as e:
        print(f"  âš ï¸  Zotero ç¬”è®°å†™å…¥å¤±è´¥: {e}")

    # 9. å†™å…¥ Zotero æ ‡ç­¾
    try:
        zotero_client.add_tags(item_key, tags)
        print(f"  âœ… Zotero æ ‡ç­¾å·²å†™å…¥: {tags}")
    except RuntimeError as e:
        print(f"  âš ï¸  {e}")
    except Exception as e:
        print(f"  âš ï¸  Zotero æ ‡ç­¾å†™å…¥å¤±è´¥: {e}")

    return True


def load_processed_ids(processed_file):
    """åŠ è½½å·²å¤„ç†çš„æ¡ç›® ID é›†åˆ"""
    if os.path.exists(processed_file):
        with open(processed_file, 'r') as f:
            return set(line.strip() for line in f if line.strip())
    return set()


def save_processed_id(processed_file, item_key):
    """è®°å½•å·²å¤„ç†çš„æ¡ç›® ID"""
    with open(processed_file, 'a') as f:
        f.write(item_key + '\n')


# ---- å‘½ä»¤è¡Œå…¥å£ ----

def main():
    parser = argparse.ArgumentParser(
        description='Zotero è®ºæ–‡è‡ªåŠ¨åˆ†æå·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python paper_analyzer.py --all                           # å¤„ç†æ‰€æœ‰æœªåˆ†æçš„æ–°è®ºæ–‡
  python paper_analyzer.py --key ABC123DE                 # å¤„ç†æŒ‡å®š Zotero æ¡ç›®
  python paper_analyzer.py --recent 5                     # å¤„ç†æœ€è¿‘5ç¯‡è®ºæ–‡
  python paper_analyzer.py --dry-run --recent 3           # é¢„è§ˆï¼ˆä¸å†™å…¥ï¼‰
  python paper_analyzer.py --recent 1 --model gpt-4o-mini # ä½¿ç”¨è½»é‡æ¨¡å‹
  python paper_analyzer.py --recent 1 --model claude-haiku-4-5    # ä½¿ç”¨ Claude Haiku
  python paper_analyzer.py --recent 1 --model claude-sonnet-4-6   # ä½¿ç”¨ Claude Sonnet 4.6
        """
    )
    parser.add_argument('--key', type=str, help='å¤„ç†æŒ‡å®šçš„ Zotero item key')
    parser.add_argument('--all', action='store_true', help='å¤„ç†æ‰€æœ‰æœªåˆ†æçš„æ–°è®ºæ–‡')
    parser.add_argument('--recent', type=int, metavar='N', help='å¤„ç†æœ€è¿‘ N ç¯‡è®ºæ–‡')
    parser.add_argument('--dry-run', action='store_true', help='é¢„è§ˆæ¨¡å¼ï¼Œä¸è°ƒç”¨ LLM ä¹Ÿä¸å†™å…¥')
    parser.add_argument('--config', type=str, help='æŒ‡å®š config.yaml è·¯å¾„')
    parser.add_argument('--model', type=str, metavar='MODEL',
                        help='è¦†ç›– config.yaml ä¸­çš„æ¨¡å‹è®¾ç½®ï¼Œå¦‚ gpt-4o / gpt-4o-mini / claude-haiku-4-5 / claude-sonnet-4-6')
    args = parser.parse_args()

    if not any([args.key, args.all, args.recent]):
        parser.print_help()
        sys.exit(1)

    # åŠ è½½é…ç½®
    if args.config:
        with open(args.config, 'r') as f:
            config = yaml.safe_load(f)
    else:
        config = load_config()

    # æ£€æŸ¥ API key é…ç½®
    if config['zotero']['api_key'] == 'YOUR_ZOTERO_API_KEY':
        print("âŒ è¯·å…ˆåœ¨ config.yaml ä¸­é…ç½® Zotero API Key")
        print("   è·å–æ–¹å¼ï¼šhttps://www.zotero.org/settings/keys")
        sys.exit(1)
    if config['github_models']['token'] == 'YOUR_GITHUB_PERSONAL_ACCESS_TOKEN':
        print("âŒ è¯·å…ˆåœ¨ config.yaml ä¸­é…ç½® GitHub Personal Access Token")
        print("   è·å–æ–¹å¼ï¼šhttps://github.com/settings/tokens")
        sys.exit(1)

    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    print("ğŸš€ åˆå§‹åŒ–å®¢æˆ·ç«¯...")
    zotero_client = ZoteroClient(config)
    llm_client = GitHubModelsClient(config, model_override=args.model)
    print(f"   ä½¿ç”¨æ¨¡å‹: {llm_client.model}")

    processed_file = config.get('watchdog', {}).get(
        'processed_ids_file',
        os.path.join(os.path.dirname(__file__), '..', '.processed_ids')
    )

    success_count = 0
    fail_count = 0
    skip_count = 0

    if args.key:
        # å¤„ç†å•ä¸ªæ¡ç›®
        ok = process_item(args.key, zotero_client, llm_client, config, dry_run=args.dry_run)
        if ok:
            save_processed_id(processed_file, args.key)
            success_count += 1
        else:
            fail_count += 1

    elif args.all:
        # å…¨åº“æ‰¹é‡å¤„ç†ï¼ˆåˆ†é¡µè·å–æ‰€æœ‰æ¡ç›®ï¼‰
        print("ğŸ“¥ æ­£åœ¨è·å– Zotero å…¨åº“æ¡ç›®ï¼ˆåˆ†é¡µåŠ è½½ï¼‰...")
        items = zotero_client.get_all_items()
        processed_ids = load_processed_ids(processed_file)
        total = len(items)
        print(f"ğŸ“š å…¨åº“å…± {total} ç¯‡æ–‡çŒ®ï¼Œå·²å¤„ç† {len(processed_ids)} ç¯‡ï¼Œ"
              f"å¾…å¤„ç† {total - len([i for i in items if i['data']['key'] in processed_ids])} ç¯‡\n")

        for idx, item in enumerate(items, 1):
            key = item['data']['key']
            title = item['data'].get('title', key)[:50]
            if key in processed_ids:
                skip_count += 1
                continue

            print(f"[{idx}/{total}] ", end='', flush=True)
            ok = process_item(key, zotero_client, llm_client, config, dry_run=args.dry_run)
            if ok:
                if not args.dry_run:
                    save_processed_id(processed_file, key)
                success_count += 1
            else:
                fail_count += 1

            # é€Ÿç‡æ§åˆ¶ï¼šæ¯ç¯‡ä¹‹é—´ç­‰å¾… 3 ç§’ï¼Œé¿å…è§¦å‘ GitHub Models rate limit
            if not args.dry_run and idx < total:
                time.sleep(3)

    elif args.recent:
        print(f"ğŸ“¥ è·å–æœ€è¿‘ {args.recent} ç¯‡è®ºæ–‡...")
        items = zotero_client.get_recent_items(limit=args.recent)
        processed_ids = load_processed_ids(processed_file)
        total = len(items)

        for idx, item in enumerate(items, 1):
            key = item['data']['key']
            print(f"[{idx}/{total}] ", end='', flush=True)
            ok = process_item(key, zotero_client, llm_client, config, dry_run=args.dry_run)
            if ok:
                if not args.dry_run:
                    save_processed_id(processed_file, key)
                success_count += 1
            else:
                fail_count += 1

            if not args.dry_run and idx < total:
                time.sleep(3)

    print(f"\n{'='*60}")
    print(f"âœ… å®Œæˆï¼æˆåŠŸ: {success_count}  è·³è¿‡: {skip_count}  å¤±è´¥: {fail_count}")


if __name__ == '__main__':
    main()
