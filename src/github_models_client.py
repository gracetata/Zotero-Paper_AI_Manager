"""
github_models_client.py â€” å¤šæ¨¡å‹ LLM å®¢æˆ·ç«¯
æ”¯æŒï¼š
  - GitHub Models (gpt-4o, gpt-4o-mini ç­‰ OpenAI å…¼å®¹æ¨¡å‹)
  - Anthropic API (claude-haiku-4-5, claude-sonnet-4-6 ç­‰)
è‡ªåŠ¨é€‰æ‹© providerï¼Œé‡åˆ° 413 æ—¶é€æ­¥æˆªæ–­é‡è¯•å¹¶æŠ¥å‘Šé˜…è¯»æ¯”ä¾‹
"""

import os
import re
import json
import requests
import yaml


def load_config(config_path=None):
    if config_path is None:
        config_path = os.path.join(os.path.dirname(__file__), '..', 'config.yaml')
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def load_skill_prompt(skill_path=None):
    """
    ä» SKILL.md æå–å®Œæ•´ System Promptï¼ˆåŒ…å«åˆ†ææ¨¡æ¿ï¼‰ã€‚
    æå–èŒƒå›´ï¼šfrontmatter ä¹‹åçš„æ‰€æœ‰å†…å®¹ã€‚
    """
    if skill_path is None:
        skill_path = os.path.join(os.path.dirname(__file__), '..', 'skills', 'read-paper', 'SKILL.md')
    with open(skill_path, 'r', encoding='utf-8') as f:
        content = f.read()
    stripped = re.sub(r'^---\n.*?\n---\n', '', content, count=1, flags=re.DOTALL)
    return stripped.strip()


# ---- Provider æ£€æµ‹ ----

def _is_anthropic_model(model_name):
    """åˆ¤æ–­æ˜¯å¦ä¸º Anthropic Claude æ¨¡å‹"""
    return model_name.startswith('claude-')


# ---- Anthropic API è°ƒç”¨ ----

def _call_anthropic(api_key, model, system_prompt, user_message, max_tokens=2048, temperature=0.3):
    """è°ƒç”¨ Anthropic APIï¼ˆclaude-haiku-4-5, claude-sonnet-4-6 ç­‰ï¼‰"""
    import anthropic
    client = anthropic.Anthropic(api_key=api_key)
    msg = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        temperature=temperature,
        system=system_prompt,
        messages=[{"role": "user", "content": user_message}]
    )
    return msg.content[0].text


# ---- GitHub Models API è°ƒç”¨ï¼ˆOpenAI å…¼å®¹ï¼‰----

def _call_github_models(token, endpoint, model, system_prompt, user_message, max_tokens=2048, temperature=0.3):
    """è°ƒç”¨ GitHub Models REST API"""
    url = f"{endpoint}/chat/completions"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_message},
        ],
        "max_tokens": max_tokens,
        "temperature": temperature,
    }
    resp = requests.post(url, headers=headers, json=payload, timeout=120)
    if resp.status_code == 413:
        raise ValueError("__413__")
    if resp.status_code == 401:
        raise RuntimeError("GitHub Token æ— æ•ˆæˆ–è¿‡æœŸï¼Œè¯·æ£€æŸ¥ config.yaml ä¸­çš„ token")
    if resp.status_code == 429:
        raise RuntimeError("GitHub Models API è¯·æ±‚é¢‘ç‡è¶…é™ï¼ˆrate limitï¼‰ï¼Œè¯·ç¨åå†è¯•")
    resp.raise_for_status()
    return resp.json()['choices'][0]['message']['content']


class GitHubModelsClient:
    def __init__(self, config=None, model_override=None):
        if config is None:
            config = load_config()
        gm_cfg = config['github_models']
        self.token = gm_cfg['token']
        self.endpoint = gm_cfg.get('endpoint', 'https://models.inference.ai.azure.com')
        self.model = model_override or gm_cfg.get('model', 'gpt-4o')
        self.max_tokens = gm_cfg.get('max_tokens', 2048)
        self.temperature = gm_cfg.get('temperature', 0.3)
        self.skill_system_prompt = load_skill_prompt()

        # Anthropic é…ç½®ï¼ˆå¯é€‰ï¼‰
        ant_cfg = config.get('anthropic', {})
        self.anthropic_key = ant_cfg.get('api_key', '')

        # é¢„å®šä¹‰æ ‡ç­¾ï¼ˆä¸¥æ ¼ç™½åå•ï¼‰
        tags_cfg = config.get('tags', {})
        self.valid_tags = (
            tags_cfg.get('domain', []) +
            tags_cfg.get('method', []) +
            tags_cfg.get('status', [])
        )

        # æ¨¡å‹åˆ‡æ¢é˜ˆå€¼
        fb_cfg = config.get('model_fallback', {})
        self.fallback_enabled = fb_cfg.get('enabled', True)
        self.fallback_threshold = fb_cfg.get('threshold_chars', 80000)
        self.fallback_model = fb_cfg.get('large_context_model', self.model)

    def analyze_paper(self, metadata, pdf_text=None, original_pdf_chars=None):
        """
        è°ƒç”¨ LLM åˆ†æè®ºæ–‡ã€‚
        ç­–ç•¥ï¼š
          - çŸ­æ–‡ï¼ˆ< CHUNK_LIMITï¼‰ï¼šå•æ¬¡æäº¤
          - é•¿æ–‡ï¼ˆ>= CHUNK_LIMITï¼‰ï¼šåˆ†å—æäº¤ï¼ˆåˆ†æå‰åŠ + ååŠï¼‰å†åˆå¹¶
          - é‡åˆ° 413 æ—¶è‡ªåŠ¨ç¼©å°å—å¤§å°é‡è¯•

        Returns:
            tuple: (analysis_text, read_ratio, actual_chars_sent)
        """
        CHUNK_LIMIT = 25000   # å•æ¬¡å®‰å…¨æäº¤ä¸Šé™ï¼ˆå­—ç¬¦ï¼‰

        original_len = original_pdf_chars or (len(pdf_text) if pdf_text else 0)

        # è‡ªåŠ¨æ¨¡å‹åˆ‡æ¢
        model_to_use = self.model
        if self.fallback_enabled and pdf_text and len(pdf_text) > self.fallback_threshold:
            if self.fallback_model != self.model:
                model_to_use = self.fallback_model
                print(f"  âš¡ è®ºæ–‡è¾ƒé•¿ï¼Œåˆ‡æ¢åˆ° {model_to_use}")

        if not pdf_text or len(pdf_text) <= CHUNK_LIMIT:
            # çŸ­æ–‡ï¼šå•æ¬¡åˆ†æ
            result, ratio, actual = self._analyze_single(
                model_to_use, metadata, pdf_text, original_len
            )
        else:
            # é•¿æ–‡ï¼šåˆ†å—åˆ†æ
            result, ratio, actual = self._analyze_chunked(
                model_to_use, metadata, pdf_text, original_len, CHUNK_LIMIT
            )

        return self._strip_code_fences(result), ratio, actual

    def _analyze_single(self, model, metadata, pdf_text, original_len):
        """å•æ¬¡æäº¤åˆ†æï¼Œé‡åˆ° 413 è‡ªåŠ¨ç¼©å°æ–‡æœ¬é‡è¯•"""
        current_text = pdf_text
        for attempt in range(4):
            user_message = self._build_user_message(metadata, current_text)
            try:
                result = self._call(model, user_message)
                actual_chars = len(current_text) if current_text else 0
                ratio = (actual_chars / original_len) if original_len > 0 else 1.0
                return result, ratio, actual_chars
            except ValueError as e:
                if '__413__' in str(e) and current_text:
                    new_len = len(current_text) // 2
                    pct = int(new_len / original_len * 100) if original_len else 50
                    print(f"  âš ï¸  è´Ÿè½½è¿‡å¤§ï¼Œç¼©å‡è‡³ {new_len:,} å­—ç¬¦ï¼ˆåŸæ–‡ {pct}%ï¼‰åé‡è¯•...")
                    current_text = current_text[:new_len]
                else:
                    raise RuntimeError(str(e))
            except Exception as e:
                raise RuntimeError(str(e))

        # å…œåº•ï¼šçº¯å…ƒæ•°æ®
        print(f"  âš ï¸  å¤šæ¬¡é‡è¯•å¤±è´¥ï¼Œæ”¹ç”¨çº¯å…ƒæ•°æ®åˆ†æ")
        result = self._call(model, self._build_user_message(metadata, None))
        return result, 0.0, 0

    def _analyze_chunked(self, model, metadata, pdf_text, original_len, chunk_limit):
        """
        åˆ†å—åˆ†æé•¿æ–‡çŒ®ï¼š
          ç¬¬1å—ï¼ˆå‰åŠï¼‰â†’ æå–é—®é¢˜/Insight/æ–¹æ³•
          ç¬¬2å—ï¼ˆååŠï¼‰â†’ æå–å®éªŒ/ç»“æœ/å±€é™
          æœ€ç»ˆåˆå¹¶     â†’ ç”Ÿæˆå®Œæ•´ç»“æ„åŒ–æŠ¥å‘Š
        """
        total_chars = len(pdf_text)
        mid = total_chars // 2
        chunk1 = pdf_text[:mid]
        chunk2 = pdf_text[mid:]

        # å¦‚æœå•å—ä»è¶…é™ï¼Œç¼©å°åˆ° chunk_limit
        if len(chunk1) > chunk_limit:
            chunk1 = chunk1[:chunk_limit]
        if len(chunk2) > chunk_limit:
            chunk2 = chunk2[-chunk_limit:]   # å–ååŠçš„æœ«å°¾ï¼ˆç»“è®ºåŒºåŸŸï¼‰

        actual_chars = len(chunk1) + len(chunk2)
        ratio = actual_chars / original_len if original_len else 1.0
        pct = int(ratio * 100)
        print(f"  ğŸ“š åˆ†å—åˆ†æï¼šå—1={len(chunk1):,}å­—ç¬¦ + å—2={len(chunk2):,}å­—ç¬¦ = {pct}% è¦†ç›–ç‡")

        # ç¬¬1å—ï¼šé—®é¢˜/èƒŒæ™¯/æ–¹æ³•
        prompt1 = (
            f"ä½ æ­£åœ¨é˜…è¯»è®ºæ–‡ã€Š{metadata.get('title','?')}ã€‹çš„ã€å‰åŠéƒ¨åˆ†ã€‘ã€‚\n"
            f"è¯·ä»…åŸºäºæ­¤éƒ¨åˆ†å†…å®¹ï¼Œæå–ï¼š\n"
            f"1. å½“å‰é¢†åŸŸçš„æ ¸å¿ƒé—®é¢˜ä¸æŒ‘æˆ˜\n"
            f"2. ä½œè€…çš„æ ¸å¿ƒ Insightï¼ˆæ´å¯Ÿï¼‰\n"
            f"3. æ–¹æ³•è®¾è®¡ï¼ˆå¦‚ä½•ç”¨ Insight è§£å†³é—®é¢˜ï¼‰\n\n"
            f"è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹ï¼Œæ ‡æ³¨ã€Œå‰åŠéƒ¨åˆ†åˆ†æã€ã€‚\n\n"
            f"--- è®ºæ–‡å‰åŠéƒ¨åˆ† ---\n{chunk1}"
        )
        print(f"  ğŸ¤– åˆ†æç¬¬1å—ï¼ˆå‰åŠï¼šé—®é¢˜/Insight/æ–¹æ³•ï¼‰...")
        analysis1 = self._call_with_retry(model, prompt1)

        # ç¬¬2å—ï¼šå®éªŒ/ç»“æœ/å±€é™
        prompt2 = (
            f"ä½ æ­£åœ¨é˜…è¯»è®ºæ–‡ã€Š{metadata.get('title','?')}ã€‹çš„ã€ååŠéƒ¨åˆ†ã€‘ã€‚\n"
            f"è¯·ä»…åŸºäºæ­¤éƒ¨åˆ†å†…å®¹ï¼Œæå–ï¼š\n"
            f"1. å®éªŒè®¾è®¡ä¸è´¡çŒ®çš„å¯¹åº”å…³ç³»ï¼ˆMetricsã€å¯¹æ¯”åŸºçº¿ï¼‰\n"
            f"2. æœ¬è´¨å¯å‘ï¼ˆå¯è¿ç§»çš„æ–¹æ³•è®ºï¼‰\n"
            f"3. å±€é™æ€§ï¼ˆæ–¹æ³•çš„æ ¹æœ¬é™åˆ¶ï¼‰\n\n"
            f"è¾“å‡ºçº¯æ–‡æœ¬ï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹ï¼Œæ ‡æ³¨ã€ŒååŠéƒ¨åˆ†åˆ†æã€ã€‚\n\n"
            f"--- è®ºæ–‡ååŠéƒ¨åˆ† ---\n{chunk2}"
        )
        print(f"  ğŸ¤– åˆ†æç¬¬2å—ï¼ˆååŠï¼šå®éªŒ/ç»“æœ/å±€é™ï¼‰...")
        analysis2 = self._call_with_retry(model, prompt2)

        # åˆå¹¶ï¼šç”Ÿæˆæœ€ç»ˆç»“æ„åŒ–æŠ¥å‘Š
        merge_prompt = (
            f"è¯·å°†ä»¥ä¸‹ä¸¤æ®µå¯¹è®ºæ–‡ã€Š{metadata.get('title','?')}ã€‹çš„åˆ†æ®µåˆ†æï¼Œ"
            f"æ•´åˆä¸ºä¸€ä»½å®Œæ•´çš„ã€ç¬¦åˆæ ¼å¼è¦æ±‚çš„è®ºæ–‡åˆ†ææŠ¥å‘Šã€‚\n"
            f"ç›´æ¥è¾“å‡ºæŠ¥å‘Šå†…å®¹ï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹ï¼Œä¸è¦æœ‰å¤šä½™å‰è¨€ã€‚\n\n"
            f"=== å‰åŠéƒ¨åˆ†åˆ†æ ===\n{analysis1}\n\n"
            f"=== ååŠéƒ¨åˆ†åˆ†æ ===\n{analysis2}"
        )
        print(f"  ğŸ¤– åˆå¹¶ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
        final = self._call_with_retry(model, merge_prompt)
        return final, ratio, actual_chars

    def _call_with_retry(self, model, user_message):
        """è°ƒç”¨ LLMï¼Œé‡åˆ° 413 ç¼©çŸ­æ¶ˆæ¯é‡è¯•"""
        for attempt in range(3):
            try:
                return self._call(model, user_message)
            except ValueError as e:
                if '__413__' in str(e):
                    # æˆªæ–­ user_message æœ«å°¾ 30%
                    user_message = user_message[:int(len(user_message) * 0.7)]
                    print(f"    â†©ï¸  è´Ÿè½½è¿‡å¤§ï¼Œç¼©å‡æ¶ˆæ¯åé‡è¯•...")
                else:
                    raise RuntimeError(str(e))
            except Exception as e:
                raise RuntimeError(str(e))
        raise RuntimeError("å¤šæ¬¡é‡è¯•åä»æ— æ³•å®Œæˆ LLM è°ƒç”¨")

    @staticmethod
    def _strip_code_fences(text):
        """ç§»é™¤ LLM è¾“å‡ºä¸­çš„ä»£ç å›´æ ï¼ˆ```markdown ... ``` ç­‰ï¼‰"""
        import re
        text = re.sub(r'^```[a-zA-Z]*\n?', '', text.strip())
        text = re.sub(r'\n?```$', '', text.strip())
        return text.strip()

    def _call(self, model, user_message):
        """æ ¹æ® model åç§°è‡ªåŠ¨é€‰æ‹© provider"""
        if _is_anthropic_model(model):
            if not self.anthropic_key:
                raise RuntimeError(
                    f"ä½¿ç”¨ Claude æ¨¡å‹éœ€è¦åœ¨ config.yaml ä¸­é…ç½® anthropic.api_key\n"
                    f"è·å–æ–¹å¼ï¼šhttps://console.anthropic.com/settings/keys"
                )
            return _call_anthropic(
                self.anthropic_key, model,
                self.skill_system_prompt, user_message,
                self.max_tokens, self.temperature
            )
        else:
            return _call_github_models(
                self.token, self.endpoint, model,
                self.skill_system_prompt, user_message,
                self.max_tokens, self.temperature
            )

    def _build_user_message(self, metadata, pdf_text):
        parts = [
            "è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ã€‚**è¾“å‡ºæ ¼å¼è¦æ±‚ï¼šç›´æ¥è¾“å‡º Markdown æ­£æ–‡ï¼Œç¦æ­¢ç”¨ä»£ç å—ï¼ˆ```ï¼‰åŒ…è£¹æ•´ä¸ªè¾“å‡ºã€‚**\n",
            f"**æ ‡é¢˜**: {metadata.get('title', 'æœªçŸ¥')}",
            f"**ä½œè€…**: {metadata.get('authors', 'æœªçŸ¥')}",
            f"**å¹´ä»½**: {metadata.get('year', 'æœªçŸ¥')}",
            f"**æœŸåˆŠ/ä¼šè®®**: {metadata.get('venue', 'æœªçŸ¥')}",
            f"**DOI**: {metadata.get('doi', 'æ— ')}",
        ]
        abstract = metadata.get('abstract', '').strip()
        if abstract:
            parts.append(f"\n**æ‘˜è¦**:\n{abstract}")
        if pdf_text:
            parts.append(f"\n---\n\n**è®ºæ–‡å…¨æ–‡**:\n\n{pdf_text}")
        else:
            parts.append('\nï¼ˆæ³¨ï¼šæœªèƒ½æå– PDF å…¨æ–‡ï¼Œè¯·ä»…åŸºäºä»¥ä¸Šå…ƒæ•°æ®è¿›è¡Œåˆ†æï¼Œå¯¹æœªçŸ¥å†…å®¹æ ‡æ³¨"åŸæ–‡æœªæåŠ"ï¼‰')

        # ä¸¥æ ¼æ ‡ç­¾çº¦æŸï¼šæ³¨å…¥å¯ç”¨æ ‡ç­¾ç™½åå•
        if self.valid_tags:
            tag_list = json.dumps(self.valid_tags, ensure_ascii=False)
            parts.append(
                f"\n---\n\n**ã€æ ‡ç­¾é€‰æ‹©â€”â€”ä¸¥æ ¼è¦æ±‚ã€‘**\n"
                f"è¯·ä»ä¸‹æ–¹ç™½åå•ä¸­é€‰å‡º 2-5 ä¸ªæœ€è´´åˆ‡çš„æ ‡ç­¾ï¼Œè¾“å‡ºä¸º JSON æ•°ç»„ã€‚\n"
                f"âš ï¸ åªèƒ½ä½¿ç”¨ç™½åå•ä¸­çš„åŸæ–‡æ ‡ç­¾ï¼Œç¦æ­¢åˆ›é€ ä»»ä½•æ–°æ ‡ç­¾ï¼Œç¦æ­¢ä¿®æ”¹æ ‡ç­¾æ–‡å­—ã€‚\n"
                f"ç™½åå•ï¼š{tag_list}\n"
                f"è¾“å‡ºæ ¼å¼ç¤ºä¾‹ï¼ˆæ”¾åœ¨åˆ†ææœ«å°¾ï¼‰ï¼š\n"
                f'**æ¨èæ ‡ç­¾**: ["å››è¶³æœºå™¨äºº", "å¼ºåŒ–å­¦ä¹ ", "çœŸå®å®éªŒ"]'
            )
        return '\n'.join(parts)

    def extract_tags_from_analysis(self, analysis_text, valid_tags=None):
        """
        ä» LLM è¾“å‡ºä¸­æå–æ ‡ç­¾ï¼Œä¸¥æ ¼è¿‡æ»¤åˆ°ç™½åå•ã€‚
        ç­–ç•¥ï¼š
          1. æ‰¾ JSON æ•°ç»„ â†’ è¿‡æ»¤åˆ° valid_tags
          2. æ‰¾ã€Œæ¨èæ ‡ç­¾:ã€è¡Œ â†’ è§£æå…¶ä¸­æ ‡ç­¾ â†’ è¿‡æ»¤åˆ° valid_tags
          3. ä»»ä½•éç™½åå•æ ‡ç­¾ç›´æ¥ä¸¢å¼ƒï¼ˆä¸åšå…³é”®è¯åŒ¹é…ï¼Œé¿å…è¯¯åˆ¤ï¼‰
        """
        whitelist = set(valid_tags or self.valid_tags)
        found_tags = []

        # ç­–ç•¥1: æ‰¾ JSON æ•°ç»„ï¼ˆå¦‚ ["A", "B", "C"]ï¼‰
        matches = re.findall(r'\[([^\[\]]{2,300})\]', analysis_text)
        for match in matches:
            try:
                tags = json.loads(f'[{match}]')
                if tags and all(isinstance(t, str) for t in tags):
                    filtered = [t.strip() for t in tags if t.strip() in whitelist]
                    if filtered:
                        found_tags = filtered
                        break
            except (json.JSONDecodeError, ValueError):
                continue

        # ç­–ç•¥2: æ‰¾ã€Œæ¨èæ ‡ç­¾ã€è¡Œï¼Œé€ä¸ªè¯åŒ¹é…ç™½åå•
        if not found_tags:
            tag_line_match = re.search(
                r'(?:æ¨èæ ‡ç­¾|å»ºè®®æ ‡ç­¾|æ ‡ç­¾)[ï¼š:]\s*(.+)', analysis_text
            )
            if tag_line_match:
                line = tag_line_match.group(1)
                # å»æ‰ markdown æ ¼å¼ï¼ŒæŒ‰å¸¸è§åˆ†éš”ç¬¦åˆ‡åˆ†
                line = re.sub(r'[`\[\]"\'ã€ã€‘]', ' ', line)
                candidates = re.split(r'[,ï¼Œã€\s]+', line)
                found_tags = [c.strip() for c in candidates if c.strip() in whitelist]

        # ç­–ç•¥3: åœ¨æ•´ä¸ªæ–‡æœ¬é‡Œé€ä¸€ç²¾ç¡®å­ä¸²åŒ¹é…ç™½åå•ï¼ˆå…œåº•ï¼‰
        # ä¸­æ–‡æ ‡ç­¾ä¸éœ€è¦è¯è¾¹ç•Œï¼Œç›´æ¥å­ä¸²åŒ¹é…å³å¯ï¼ˆæ ‡ç­¾æœ¬èº«éƒ½æ˜¯ä¸“ä¸šè¯æ±‡ï¼Œè¯¯åˆ¤ç‡æä½ï¼‰
        if not found_tags and whitelist:
            for tag in whitelist:
                if tag in analysis_text:
                    found_tags.append(tag)
            found_tags = found_tags[:5]  # æœ€å¤š5ä¸ª

        return found_tags
