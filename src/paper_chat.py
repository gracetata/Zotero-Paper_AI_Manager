"""
paper_chat.py â€” è®ºæ–‡ AI è¿½é—®å¯¹è¯æ¨¡å¼

åŸºäºå·²æœ‰è®ºæ–‡åˆ†æåŠ è½½ä¸Šä¸‹æ–‡ï¼Œåœ¨ç»ˆç«¯è¿›è¡Œå¤šè½®è¿½é—®ã€‚
ç”¨æ³•:
  python paper_chat.py --key ZOTERO_ITEM_KEY     # ç”¨ Zotero key åŠ è½½è®ºæ–‡
  python paper_chat.py --md path/to/note.md      # ç›´æ¥åŠ è½½ Markdown åˆ†ææ–‡ä»¶
  python paper_chat.py --key ITEM_KEY --model claude-haiku-4-5  # æŒ‡å®šæ¨¡å‹
"""

import os
import sys
import re
import yaml
import argparse

sys.path.insert(0, os.path.dirname(__file__))

from zotero_client import ZoteroClient
from pdf_extractor import extract_all_pages
from github_models_client import GitHubModelsClient


WELCOME = """
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              ğŸ“–  è®ºæ–‡è¿½é—®æ¨¡å¼  Paper Chat                    â•‘
â•‘  è¾“å…¥ä½ çš„é—®é¢˜ï¼Œç›´æ¥å›è½¦æäº¤ï¼›è¾“å…¥ q / exit é€€å‡º              â•‘
â•‘  è¾“å…¥ /clear æ¸…ç©ºå¯¹è¯å†å²ï¼›è¾“å…¥ /info æŸ¥çœ‹è®ºæ–‡ä¿¡æ¯           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""


def load_config(config_path=None):
    if config_path is None:
        config_path = os.path.join(os.path.dirname(__file__), '..', 'config.yaml')
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def find_markdown_for_key(item_key, notes_dir):
    """åœ¨ notes/ ç›®å½•ä¸‹æœç´¢åŒ…å«æŒ‡å®š zotero_key çš„ Markdown æ–‡ä»¶"""
    for root, dirs, files in os.walk(notes_dir):
        for fname in files:
            if not fname.endswith('.md') or fname == 'INDEX.md':
                continue
            fpath = os.path.join(root, fname)
            with open(fpath, 'r', encoding='utf-8') as f:
                head = f.read(512)
            if f'zotero_key: {item_key}' in head:
                return fpath
    return None


def load_context_from_markdown(md_path):
    """ä» Markdown æ–‡ä»¶æå– frontmatter å…ƒæ•°æ®å’Œåˆ†æå†…å®¹"""
    with open(md_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå– frontmatter
    metadata = {}
    fm_match = re.match(r'^---\n(.*?)\n---\n', content, re.DOTALL)
    if fm_match:
        for line in fm_match.group(1).split('\n'):
            if ':' in line:
                k, v = line.split(':', 1)
                metadata[k.strip()] = v.strip().strip('"')
        body = content[fm_match.end():]
    else:
        body = content

    return metadata, body


def build_system_prompt(metadata, prior_analysis, pdf_text=None):
    """æ„å»ºå¯¹è¯ system promptï¼ŒåŒ…å«è®ºæ–‡ä¸Šä¸‹æ–‡"""
    parts = [
        "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å­¦æœ¯è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚ç”¨æˆ·å°†å°±ä»¥ä¸‹è®ºæ–‡å‘ä½ æé—®ï¼Œè¯·åŸºäºè®ºæ–‡å†…å®¹å’Œå·²æœ‰åˆ†æç»™å‡ºç²¾å‡†å›ç­”ã€‚",
        "",
        "ã€è®ºæ–‡åŸºæœ¬ä¿¡æ¯ã€‘",
        f"æ ‡é¢˜: {metadata.get('title', 'æœªçŸ¥')}",
        f"ä½œè€…: {metadata.get('authors', 'æœªçŸ¥')}",
        f"å¹´ä»½: {metadata.get('year', 'æœªçŸ¥')}",
        f"æœŸåˆŠ/ä¼šè®®: {metadata.get('venue', 'æœªçŸ¥')}",
        "",
    ]
    if prior_analysis:
        parts += [
            "ã€å·²æœ‰ AI åˆ†ææ‘˜è¦ï¼ˆå¯ä¾›å‚è€ƒï¼‰ã€‘",
            prior_analysis[:3000],  # é˜²æ­¢ system prompt è¿‡é•¿
            "",
        ]
    if pdf_text:
        parts += [
            "ã€è®ºæ–‡å…¨æ–‡ï¼ˆéƒ¨åˆ†ï¼‰ã€‘",
            pdf_text[:8000],
            "",
        ]
    parts.append(
        "è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚å¦‚æœé—®é¢˜è¶…å‡ºè®ºæ–‡èŒƒå›´ï¼Œè¯·è¯šå®è¯´æ˜ï¼Œä¸è¦æé€ å†…å®¹ã€‚"
    )
    return '\n'.join(parts)


def chat_loop(system_prompt, llm_client, metadata):
    """å¤šè½®å¯¹è¯ä¸»å¾ªç¯"""
    print(WELCOME)
    print(f"ğŸ“„ è®ºæ–‡: {metadata.get('title', '?')[:80]}")
    print(f"ğŸ‘¤ ä½œè€…: {metadata.get('authors', '?')[:60]}")
    print(f"ğŸ¤– æ¨¡å‹: {llm_client.model}")
    print()

    conversation = []  # å¯¹è¯å†å² [{"role": ..., "content": ...}]

    while True:
        try:
            user_input = input("ä½ : ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n\nğŸ‘‹ é€€å‡ºè¿½é—®æ¨¡å¼")
            break

        if not user_input:
            continue
        if user_input.lower() in ('q', 'exit', 'quit', 'é€€å‡º'):
            print("ğŸ‘‹ é€€å‡ºè¿½é—®æ¨¡å¼")
            break
        if user_input == '/clear':
            conversation.clear()
            print("âœ… å¯¹è¯å†å²å·²æ¸…ç©º\n")
            continue
        if user_input == '/info':
            print(f"\nğŸ“„ æ ‡é¢˜: {metadata.get('title', '?')}")
            print(f"ğŸ‘¤ ä½œè€…: {metadata.get('authors', '?')}")
            print(f"ğŸ“… å¹´ä»½: {metadata.get('year', '?')}")
            print(f"ğŸ›ï¸  æœŸåˆŠ: {metadata.get('venue', '?')}")
            print(f"ğŸ¤– æ¨¡å‹: {llm_client.model}\n")
            continue

        conversation.append({"role": "user", "content": user_input})

        # è°ƒç”¨ LLMï¼ˆå¸¦å¯¹è¯å†å²ï¼‰
        print("\nğŸ¤– AI: ", end='', flush=True)
        try:
            reply = _call_with_history(llm_client, system_prompt, conversation)
            print(reply)
            print()
            conversation.append({"role": "assistant", "content": reply})
        except Exception as e:
            print(f"\nâŒ è°ƒç”¨å¤±è´¥: {e}\n")
            conversation.pop()  # å¤±è´¥æ—¶ç§»é™¤ç”¨æˆ·æ¶ˆæ¯


def _call_with_history(llm_client, system_prompt, conversation):
    """è°ƒç”¨ LLMï¼Œæ”¯æŒå¤šè½®å¯¹è¯å†å²"""
    from github_models_client import _is_anthropic_model

    model = llm_client.model

    if _is_anthropic_model(model):
        import anthropic
        if not llm_client.anthropic_key:
            raise RuntimeError("éœ€è¦åœ¨ config.yaml ä¸­é…ç½® anthropic.api_key æ‰èƒ½ä½¿ç”¨ Claude æ¨¡å‹")
        client = anthropic.Anthropic(api_key=llm_client.anthropic_key)
        msg = client.messages.create(
            model=model,
            max_tokens=1024,
            temperature=0.3,
            system=system_prompt,
            messages=conversation,
        )
        return msg.content[0].text
    else:
        import requests as req
        url = f"{llm_client.endpoint}/chat/completions"
        headers = {"Authorization": f"Bearer {llm_client.token}", "Content-Type": "application/json"}
        messages = [{"role": "system", "content": system_prompt}] + conversation
        payload = {
            "model": model,
            "messages": messages,
            "max_tokens": 1024,
            "temperature": 0.3,
        }
        resp = req.post(url, headers=headers, json=payload, timeout=60)
        if resp.status_code == 401:
            raise RuntimeError("GitHub Token æ— æ•ˆ")
        resp.raise_for_status()
        return resp.json()['choices'][0]['message']['content']


def main():
    parser = argparse.ArgumentParser(
        description='è®ºæ–‡ AI è¿½é—®å¯¹è¯',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  python paper_chat.py --key ABCD1234
  python paper_chat.py --md ../notes/2024/MyPaper.md
  python paper_chat.py --key ABCD1234 --model claude-haiku-4-5
  python paper_chat.py --key ABCD1234 --no-pdf    # ä¸åŠ è½½ PDFï¼Œåªç”¨å·²æœ‰åˆ†æ
        """
    )
    parser.add_argument('--key', type=str, help='Zotero item key')
    parser.add_argument('--md', type=str, help='ç›´æ¥æŒ‡å®šå·²æœ‰åˆ†æ Markdown æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--model', type=str, help='æŒ‡å®šæ¨¡å‹ï¼ˆå¦‚ claude-haiku-4-5, gpt-4oï¼‰')
    parser.add_argument('--no-pdf', action='store_true', help='ä¸åŠ è½½ PDF å…¨æ–‡ï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰')
    parser.add_argument('--config', type=str, help='æŒ‡å®š config.yaml è·¯å¾„')
    args = parser.parse_args()

    if not args.key and not args.md:
        parser.print_help()
        sys.exit(1)

    # åŠ è½½é…ç½®
    if args.config:
        with open(args.config) as f:
            config = yaml.safe_load(f)
    else:
        config = load_config()

    notes_dir = config['output']['notes_dir']

    # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
    llm_client = GitHubModelsClient(config, model_override=args.model)

    # åŠ è½½ Markdown åˆ†æ
    md_path = args.md
    if not md_path and args.key:
        md_path = find_markdown_for_key(args.key, notes_dir)
        if md_path:
            print(f"âœ… æ‰¾åˆ°å·²æœ‰åˆ†æ: {md_path}")
        else:
            print(f"â„¹ï¸  æœªæ‰¾åˆ°å·²æœ‰åˆ†ææ–‡ä»¶ï¼Œå°†ç›´æ¥åŠ è½½ PDF å’Œå…ƒæ•°æ®")

    metadata = {}
    prior_analysis = ''
    if md_path and os.path.exists(md_path):
        metadata, prior_analysis = load_context_from_markdown(md_path)

    # å¦‚æœæœ‰ Zotero keyï¼Œè¡¥å……å…ƒæ•°æ®
    if args.key and not metadata.get('title'):
        print("ğŸ” ä» Zotero è·å–å…ƒæ•°æ®...")
        try:
            zotero_client = ZoteroClient(config)
            item = zotero_client.get_item(args.key)
            metadata = zotero_client.get_item_metadata(item)
        except Exception as e:
            print(f"âš ï¸  è·å– Zotero å…ƒæ•°æ®å¤±è´¥: {e}")

    # å¯é€‰ï¼šåŠ è½½ PDF å…¨æ–‡
    pdf_text = None
    if not args.no_pdf and args.key:
        try:
            zotero_client = ZoteroClient(config)
            pdf_path = zotero_client.find_local_pdf(args.key)
            if not pdf_path:
                pdf_path = zotero_client.find_pdf_via_attachments(args.key)
            if pdf_path:
                pdf_cfg = config.get('pdf', {})
                pdf_text, pages, _ = extract_all_pages(pdf_path, max_chars=pdf_cfg.get('max_chars', 150000))
                print(f"ğŸ“– PDF å·²åŠ è½½: {pages} é¡µï¼Œ{len(pdf_text):,} å­—ç¬¦")
        except Exception as e:
            print(f"âš ï¸  PDF åŠ è½½å¤±è´¥ï¼ˆå¯¹è¯ä»å¯ç»§ç»­ï¼‰: {e}")

    if not metadata.get('title') and not prior_analysis:
        print("âŒ æ— æ³•åŠ è½½è®ºæ–‡ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥ --key æˆ– --md å‚æ•°")
        sys.exit(1)

    # æ„å»º system prompt å¹¶å¼€å§‹å¯¹è¯
    system_prompt = build_system_prompt(metadata, prior_analysis, pdf_text)
    chat_loop(system_prompt, llm_client, metadata)


if __name__ == '__main__':
    main()
